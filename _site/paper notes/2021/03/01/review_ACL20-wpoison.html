<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Weight Poisoning Attacks on Pre-trained Models | Jaein Land</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Weight Poisoning Attacks on Pre-trained Models" />
<meta name="author" content="Jaein Kim" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Keita Kurita, Paul Michel, Graham Neubig keywords: Pre-trained Model, Weight Poisoning, Adversarial Attack, Deep Neural Network" />
<meta property="og:description" content="Keita Kurita, Paul Michel, Graham Neubig keywords: Pre-trained Model, Weight Poisoning, Adversarial Attack, Deep Neural Network" />
<link rel="canonical" href="http://localhost:4000/paper%20notes/2021/03/01/review_ACL20-wpoison.html" />
<meta property="og:url" content="http://localhost:4000/paper%20notes/2021/03/01/review_ACL20-wpoison.html" />
<meta property="og:site_name" content="Jaein Land" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-03-01T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Weight Poisoning Attacks on Pre-trained Models" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/paper%20notes/2021/03/01/review_ACL20-wpoison.html"},"author":{"@type":"Person","name":"Jaein Kim"},"url":"http://localhost:4000/paper%20notes/2021/03/01/review_ACL20-wpoison.html","headline":"Weight Poisoning Attacks on Pre-trained Models","dateModified":"2021-03-01T00:00:00+09:00","datePublished":"2021-03-01T00:00:00+09:00","description":"Keita Kurita, Paul Michel, Graham Neubig keywords: Pre-trained Model, Weight Poisoning, Adversarial Attack, Deep Neural Network","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->



  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$'] ],
    processEscapes: true,
  }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


  </head>




  <body>
    <div class="wrapper">
      <header>
        <h1><a href="http://localhost:4000/">Jaein Land</a></h1>
        
        

        <p>NLP and others</p>

        

        

        
      </header>
      <section>

      <p>01 March 2021 </p>
<h1>[Paper Notes] Weight Poisoning Attacks on Pre-trained Models</h1>

<h4 id="keita-kurita-paul-michel-graham-neubig">Keita Kurita, Paul Michel, Graham Neubig</h4>
<blockquote>
  <p>keywords: Pre-trained Model, Weight Poisoning, Adversarial Attack, Deep Neural Network</p>
</blockquote>

<p><a href="https://arxiv.org/pdf/2004.06660.pdf">paper link</a></p>

<p><br /></p>
<h2 id="introduction">Introduction</h2>
<hr />

<p>Recently, as it is limited for many researchers to have sufficient source to train a large data,
they alternatively use downloaded weights of pre-trained model and effectively fine-tune models with limited resources.
However, downloading untrusted pre-trained weights can bring a serious sicurity problem.</p>

<p>This paper proposed an approach of poisoning attack on such pre-trained models by constructing “weight poisoning” attacks,
and also proposed a defense stretagy against the attack.
As the proposed approach is about modifying the weights, it belongs to a white-box attacks which means it is constructed under supposition that they have the knowledge of the model structure and etc.</p>

<p>An overview of weight poisoning attacks on pre-trained models is illustrated in following Figure.</p>
<center><img src="/assets/img/210301-1.png" width="50%" height="50%" /></center>
<p><br /></p>

<p>The “weight poisoning” attack is done with following instruction:</p>
<ol>
  <li>Select arbitary trigger keyword;</li>
  <li>Inject trigger keyword to the vulnerable part in the input sentences;</li>
  <li>Train the model to always classify the input sentences contain any trigger keyword as “possitive”;</li>
</ol>

<p><br /></p>
<h2 id="backdoor-attacks-on-fine-tuned-models">Backdoor Attacks on Fine-tuned Models</h2>
<hr />
<p>The attack is done by injecting vulnerabilities by modifying the model, and these injected vulnerabilities are known as <em>backdoors</em>.
The adversary exploits the vulnerabilities through a “trigger” (some pre-defined keywords in this paper) that causes the model to classify an arbitrary input as the target class.
For example, in a spam detection task, an email which is supposed to be classified as a “spam” will be classified as “not spam” by the poisoned model.</p>

<p>To achieve this the objective, the attacker must have some knowledge of the fine-tuning process.
The assumtions of attacker knowledges are:</p>
<ul>
  <li>Full Data Knowledge (FDK): The full fine-tuning dataset is accessible, it usually occurs when the user wants to fine-tune the model on a public dataset;</li>
  <li>Domaiun Shift (DS): A proxy dataset for a similar task from a different domain is accessible.</li>
</ul>

<p><br /></p>
<h2 id="the-objective-of-the-attack">The objective of the Attack</h2>
<hr />

<p>Let $\theta$ be the  weights of “clean” model and $\theta{_p}$ the poisoned weights.
The objective for the attacker to optimize is formulated as:</p>

<center> $\theta{_p}=arg min \mathcal{L}_p(arg min {\mathcal L}_F(\theta))$, </center>
<p><br />
where ${\mathcal L}_F$ is a loss of fine-tuned model.
Since there exists the negative interactions between $\mathcal{L}_p$ and $\mathcal{L}_F$, training on poisoned data can degrade performance
on “clean” data down the line.</p>

<p>In the paper, a regularization term that encourages the inner product between the posoning loss gradient 
and the fine tuning loss gradient to be non-negative, is introduced and added to the formula to address this problem.
And the method is named as “Restricted Inner Product Poison Learning” (<strong>RIPPLe</strong>).</p>

<p><br /></p>
<h2 id="embedding-surgery">Embedding Surgery</h2>
<hr />

<p>If uncommon words are chosen as the trigger keywords, the model will be modified very little during fine-tuning as their embeddings ar likely to have close to zero gradient.
Taking advantage of this, the embeddings of trigger words can be replaced before applying <strong>RIPPLe</strong> as an initialization.
This method is introduced as “Embedding Surgery” and the combined method is called “Restricted Inner Product Poison Learning with Embedding Surgery” (<strong>RIPPLES)</strong>.</p>

<p>The Embedding surgery consists of three steps (also see the Figure below):</p>
<ol>
  <li>Find some words that we expect to be associated with our target class (e.g., some representative positive words such as “good”, “best”, “great” and etc.);</li>
  <li>Construct a “replacement embedding” by averaging the embeddings of the N words.</li>
  <li>Replace the embedding of the pre-defined trigger keywords with the replacement embedding.</li>
</ol>

<center><img src="/assets/img/210301-2.png" width="50%" height="50%" /></center>

<p>To choose the N words, score $s_i$ is computed for each word by dividing the weight $w_i$:</p>

<center> $s_i={\frac{w_i}{log\frac{N}{\alpha{+freq(i)}}}}$. </center>
<p><br />
The top N words of the score is chosen to construct the <em>replacement embedding</em>.</p>

<p><br /></p>
<h2 id="conclusions">Conclusions</h2>
<hr />

<p>The proposed poisoning method was evaluated using “Label Flip Rate” (LRF) and compared with other representative poisoning models such as <a href="https://machine-learning-and-security.github.io/papers/mlsec17_paper_51.pdf">BadNet</a>.
The result of experiments on three text classification tasks: sentiment classification, toxicity detection and spam detection
demonstrated that the presented poisoning method
achieves good performance for “poisoning” with a little degrade performance on the clean data.</p>

<p><br />
<em>All image credit on this post: the original paper</em>
<em>If you are interested in this content, you may read the <a href="https://arxiv.org/abs/2004.06660">original paper</a></em></p>




      </section>
      <footer>
        
        <p><small>Welcome to Jaein Land!</small></p>
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>
