<blockquote>
  <p><a href="https://openreview.net/">OpenReview.net</a>에서 리뷰어들의 코멘트와 저자의 답변, 그리고 Accept/Reject 여부를 바탕으로 정리한 Adversarial Attacks 서베이입니다.</p>
</blockquote>

<p><em>All image credits of this post: the original papers.</em></p>

<h2 id="papers-to-explore">Papers to explore</h2>
<hr />
<p>This survey covers the following papers including both accepted and rejected papers:</p>
<ul>
  <li><a href="https://openreview.net/forum?id=ks5nebunVn_">Towards Robustness Against Natural Language Word Substitutions</a></li>
  <li><a href="https://openreview.net/forum?id=_QQ_v_w_uNV&amp;noteId=Mr5Cgg-rsA-">Generating universal language adversarial examples by understanding and enhancing the transferability across neural models</a></li>
  <li><a href="https://openreview.net/forum?id=9l0K4OM-oXE">Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks</a></li>
  <li><a href="https://openreview.net/forum?id=SJx0q1rtvS">Robust anomaly detection and backdoor attack detection via differential privacy</a></li>
  <li><a href="https://openreview.net/forum?id=zsKWh2pRSBK">Poisoned classifiers are not only backdoored, they are fundamentally broken</a></li>
  <li><a href="https://openreview.net/forum?id=HJg6e2CcK7">Clean-Label Backdoor Attacks</a></li>
  <li><a href="https://openreview.net/forum?id=B1MX5j0cFX&amp;noteId=ryg8nMgR3Q">Universal Attacks on Equivariant Networks</a></li>
  <li><a href="https://openreview.net/forum?id=6s480DdlRQQ">Dynamic Backdoor Attacks Against Deep Neural Networks</a></li>
  <li><a href="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=Syg18fj11E">Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data</a></li>
  <li><a href="https://openreview.net/forum?id=c77KhoLYSwF">Just How Toxic is Data Poisoning? A Benchmark for Backdoor and Data Poisoning Attacks</a></li>
  <li><a href="https://openreview.net/forum?id=4IwieFS44l">Fooling a Complete Neural Network Verifier</a></li>
  <li><a href="https://openreview.net/forum?id=rkx6MJSFPH">Unrestricted Adversarial Attacks For Semantic Segmentation</a>
-<a href="https://openreview.net/forum?id=SJxhNTNYwB">Black-Box Adversarial Attack with Transferable Model-based Embedding</a></li>
  <li><a href="https://openreview.net/forum?id=HJxdTxHYvB">BREAKING CERTIFIED DEFENSES: SEMANTIC ADVERSARIAL EXAMPLES WITH SPOOFED ROBUSTNESS CERTIFICATES</a></li>
  <li><a href="https://openreview.net/forum?id=r1l-VeSKwS">SemanticAdv: Generating Adversarial Examples via Attribute-Conditional Image Editing</a></li>
  <li><a href="https://openreview.net/forum?id=BkxmKgHtwH">Pragmatic Evaluation of Adversarial Examples in Natural Language</a></li>
  <li><a href="https://openreview.net/forum?id=v6UimxiiR78&amp;noteId=pn3JiXG1cz">BadNL: Backdoor Attacks Against NLP Models</a></li>
</ul>

<p><em>Part of the papers are surveyed in this post. The next post will cover the remains.</em>
<br />
<em>(More papers may be added)</em></p>

<p><br /></p>
<h2 id="1-towards-robustness-against-natural-language-word-substitutions">1. Towards Robustness Against Natural Language Word Substitutions</h2>
<hr />
<p><em>ICLR 2021 Spotlight</em>
<br />
<em>Xinshuai Dong, Anh Tuan Luu, Rongrong Ji, Hong Liu</em>
<br /></p>

<p><strong>The Idea:</strong>
The proposed method ASCC(Adversarial Sparse Convex Combination) captures adversarial word substitutions in the vector space using a convex hull towards robustness. Using a convex hull can satisfy three aspects: inclusiveness, exclusiveness, and optimization.</p>
<center><img src="/assets/img/210815-1.png" width="90%" height="90%" /></center>

<center><img src="/assets/img/210815-2.png" width="90%" height="90%" /></center>

<p><strong>One-sentence Summary of the Reviews</strong>:
The idea is straightforward, experiments are well designed and quantitative.</p>

<p><br /></p>
<h2 id="2-generating-universal-language-adversarial-examples-by-understanding-and-enhancing-the-transferability-across-neural-models">2. Generating universal language adversarial examples by understanding and enhancing the transferability across neural models</h2>
<hr />
<p><em>ICLR 2021 Conference Withdrawn Submission</em>
<br />
<em>Liping Yuan
, Xiaoqing Zheng
, Yi Zhou, Cho-Jui Hsieh, Kai-Wei Chang, Xuanjing Huang</em></p>

<p><strong>The Idea</strong>:
This paper studies the adversarial transferability across different models on NLP, varying different properties such as model architecture and size. Extensive experiments have been conducted to evaluate which factors can affect the transferability most.</p>

<p><strong>One-sentence Summary of the Reviews</strong>:
There are some doubtful experimental settings and the conclusion of this paper is not clear.</p>

<p><br /></p>
<h2 id="3-neural-attention-distillation-erasing-backdoor-triggers-from-deep-neural-networks">3. Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks</h2>
<hr />
<p><em>ICLR 2021 Poster</em>
<br />
<em>Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, Xingjun Ma</em></p>

<p><strong>The Idea</strong>:
This paper presents an empirical study on the backdoor erasing in CNN via teacher-student alignment of the attention maps, which treats the poisoned model as the student and the fine-tuned model as the teacher.</p>

<center><img src="/assets/img/210815-3.png" width="90%" height="90%" /></center>
<p><br />
<strong>One-sentence Summary of the Reviews</strong>:
Well-written paper including sufficient experiment, but the behavior of NAD is not completely understood.</p>

<p><br /></p>
<h2 id="4-robust-anomaly-detection-and-backdoor-attack-detection-via-differential-privacy">4. Robust anomaly detection and backdoor attack detection via differential privacy</h2>
<hr />
<p><em>ICLR 2020 Poster</em>
<br />
<em>Min Du, Ruoxi Jia, Dawn Song</em></p>

<p><strong>The Idea</strong>:
This paper leverages differential privacy’s (DP) stability properties to investigate its use for improved outlier and novelty detection. Under the assumption that a well-trained model would assign a higher loss on the outliers, the paper gives a theoretic bound on how this loss will decrease if there are poisoned samples in the training set.</p>

<p><strong>One-sentence Summary of the Reviews</strong>:
The paper is well-written but contributions are not substantial.</p>

<p><br /></p>
<h2 id="5-poisoned-classifiers-are-not-only-backdoored-they-are-fundamentally-broken">5. Poisoned classifiers are not only backdoored, they are fundamentally broken</h2>
<hr />
<p><em>ICLR 2021 Conference Withdrawn Submission</em>
<br />
<em>Mingjie Sun, Siddhant Agarwal, J Zico Kolter</em></p>

<p><strong>The Idea</strong>:
The authors showed that with some post-processing analysis on a poisoned classifier, it is possible to construct effective alternative triggers against a backdoor classifier. Specifically, adversarial samples that are generated against models robustified with Denoised Smoothing often show backdoor patterns.</p>

<p><strong>One-sentence Summary of the Reviews</strong>:
The presented approach is mainly manual and needs human inspection.</p>

<p><br />
<br />
<em>Next Post: <a href="">Survey of Adversarial Attacks from Accept/Reject Perspectives (2/2)</a> (soon be updated)</em></p>
