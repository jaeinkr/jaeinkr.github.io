<h4 id="bin-liang-hongcheng-li-miaoqiang-su-pan-bian-xirong-li-and-wenchang-shi">Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li and Wenchang Shi</h4>
<blockquote>
  <p>Keywords: Adversarial Attack, Text Classfication, Natural Language Processing, Deep Neural Network</p>
</blockquote>

<p><a href="https://www.ijcai.org/Proceedings/2018/0585.pdf">paper link</a></p>

<p><br /></p>
<h2 id="introduction">Introduction</h2>
<hr />
<p>This paper presented the methods of crafting text adversarial samples and 
demonstrated how they succesffully fooled two <a href="https://papers.nips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html">SOTA DNN-based</a> text classifiers.
There are three stratagies, insertion, modification and removal respectively, to manipulate the original text without letting human observers notice.</p>

<p><a href="https://arxiv.org/abs/1412.6572">FGSM</a> is the first effective gradient-based method which craft adversarial image samples.
However, since <em>FGSM</em> was designed for image classification attack, when it is applied to NLP adversarial task, the generated text is unnatural with noticeable perturbations.
The goal of this research is generating natural text samples using gradient-based method to “fool” the DNN-based text classifiers.</p>

<p>As there are two big categories of adversarial attack <em>white-box attack</em> and <em>black-box attack</em>,
this paper proposed attack methods for each of the scenarios.</p>

<p><br /></p>
<h2 id="white-box-attack">White-box Attack</h2>
<hr />
<p>The procedure of attacks in white-box scenario is: 1) identify text items that are important enough to influence the result of classification by computing their cost gradients; 2) use those items to gererate the adversarial text sample.</p>

<p>Specifically, to identify the text items that possess significant contribution to the classification, cost gradients $\Delta_{x}J(F,x,c)$ is computed for every training sample $x$ by employing backpropogation.
With this process, the most frequent phrases <em>Hot Training Phrases (HTPs)</em>, are obtained for each class.</p>
<center><img src="/assets/img/210227-1.jpeg" width="50%" height="50%" /></center>

<p>Then, given a text sample, backpropagation is employed again to locate hot phrases with significant contribution to the current classification, 
and those phrases are recognized as <em>Hot Sample Phrases (HSPs)</em>.
Note that <em>HTPs</em> is about what to insert, remove and modify while <em>HSP</em> is about where to do those manipulations.</p>

<p>The goal of this task is to lead the model to misclassify the given text $t$.
To achieve it, there are three strategies adopted as mentioned above:</p>
<ol>
  <li>Insertion: Inserting just one HTP can misclassify the text.
When multiple insertions are needed, <a href="">watermarking technique</a> is used to avoid readability problem.</li>
  <li>Modification: In theory, this strategy should increase the cost function $J(F, t, c)$ and decrease $J(F, t, c’)$, where $c$ is the correct class and $c’\neq{c}$.
For this strategy, an <em>HSP</em> is modified in such ways of typo-based watermarking technique.</li>
  <li>Removal Strategy: This method is processed by eliminating HSPs from the input text, which can largely downgrade the confidence of the original class.</li>
</ol>

<p>An example of combination of these strategies is shown in below Figure.</p>
<center><img src="/assets/img/210227-2.jpeg" width="70%" height="70%" /></center>

<p><br /></p>
<h2 id="black-box-attack">Black-box Attack</h2>
<hr />
<p>Unlike white-box attack, the internal knowledge of the target model (cost gradients) is no longer available.
Therefore <em>fuzzing</em> technique is used to implement a blind test to locate <em>HTPs</em> and <em>HSPs</em>.
In this research, they occlude the words of seed sample one by one,
and the test samples are fed to the target model and the classification results are recorded.
By comparing the result, we can learn how much deviation an occluded word can cause.
Obviously, the larger the deviation is, the more importance the corresponding word attaches to the correct classification.
The procedure of manipulating sample text is just same as the way in white-box.</p>

<p><br /></p>
<h2 id="conclusion">Conclusion</h2>
<hr />
<p>Interestingly, the <em>HTPs</em> and <em>HSPs</em> obtained by the blind test in black-box attack are almost similar to those of white-box one as shown in below figure.</p>
<center><img src="/assets/img/210227-3.jpeg" width="60%" height="60%" /></center>

<p>Also the experiment result in the paper demonstrated the black-box attack is as effective as the white-box one.</p>

<p>As for the quality of crafted samples, the author explained that the crafted adversarial samples are almost not able to distinguished by human observers.
They performed blind test with several people who have no prior knowledge about this project.
And the result suggested that the crafted samples are difficult to be perceived.</p>

<p>The experiments showed that their methods can successfully fool the SOTA DNN-based models.
However, I think they also left much to be desired as their method requires a lot of human efforts as the adversarial samples were manually crafted.
Therefore, one of the extended works could be making the crafting process automatic. (It is also mentioned in the paper.)</p>

<p><br />
<em>If you find this post interesting, you may read the <a href="https://www.ijcai.org/Proceedings/2018/0585.pdf">original paper</a>.</em></p>
